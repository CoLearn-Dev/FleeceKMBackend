{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.future import select\n",
    "from statsmodels.api import OLS\n",
    "import pandas as pd\n",
    "from sqlalchemy import case, func\n",
    "from fleecekmbackend.db.models import Author, Answer, Rating\n",
    "from fleecekmbackend.core.config import DATABASE_URL\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setting up the async engine and session\n",
    "engine = create_async_engine(DATABASE_URL, echo=False)\n",
    "async_session = sessionmaker(engine, expire_on_commit=False, class_=AsyncSession)\n",
    "\n",
    "async def fetch_data(model_name):\n",
    "    async with async_session() as session:\n",
    "        # Define a subquery for authors using the specific model\n",
    "        author_subquery = select(Author.id).where(Author.model == model_name).subquery()\n",
    "\n",
    "        # Define a subquery for questions having both 'ic' and 'zs' answers from the same author\n",
    "        answer_subquery = select(Answer.question_id).join(\n",
    "            author_subquery, Answer.author_id == author_subquery.c.id\n",
    "        ).group_by(\n",
    "            Answer.question_id, Answer.author_id\n",
    "        ).having(\n",
    "            func.count(Answer.id) >= 2,  # Assuming at least one 'ic' and one 'zs'\n",
    "            func.sum(case((Answer.setting == 'ic', 1), else_=0)) > 0,\n",
    "            func.sum(case((Answer.setting == 'zs', 1), else_=0)) > 0\n",
    "        ).subquery()\n",
    "\n",
    "        # Main query to select answers and their ratings\n",
    "        results = await session.execute(\n",
    "            select(Answer.question_id, Answer.setting, Rating.value)\n",
    "            .join(Rating, Answer.id == Rating.answer_id)\n",
    "            .join(answer_subquery, Answer.question_id == answer_subquery.c.question_id)\n",
    "            .select_from(Answer)  # Explicitly start from Answer table\n",
    "            .limit(100)  # Adjust as needed based on expected pairs\n",
    "        )\n",
    "        return results.all()\n",
    "\n",
    "def prepare_data(data):\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame([{\n",
    "        'question_id': question_id,\n",
    "        'setting': setting,\n",
    "        'rating_value': value\n",
    "    } for question_id, setting, value in data])\n",
    "    \n",
    "    # Convert 'setting' to a binary indicator variable for regression analysis\n",
    "    df['ic'] = (df['setting'] == 'ic').astype(int)\n",
    "    return df\n",
    "\n",
    "async def analyze_model(model_name):\n",
    "    data = await fetch_data(model_name)\n",
    "    prepared_data = prepare_data(data)\n",
    "    \n",
    "    # Perform regression analysis\n",
    "    Y = prepared_data['rating_value']\n",
    "    X = prepared_data[['ic']]\n",
    "    X = sm.add_constant(X)  # adding a constant\n",
    "    model = OLS(Y, X).fit()\n",
    "    print(f\"Regression Analysis for {model_name}\")\n",
    "    print(model.summary())\n",
    "    \n",
    "    # Plotting the data\n",
    "    prepared_data.groupby(['setting'])['rating_value'].mean().plot(kind='bar')\n",
    "    plt.title(f'Average Rating by Setting for {model_name}')\n",
    "    plt.xlabel('Setting')\n",
    "    plt.ylabel('Average Rating')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(9, '<str>Answer the following question in a succinct manner: <str>', 'gpt-4-turbo')]\n"
     ]
    }
   ],
   "source": [
    "async def test_author_subquery(model_name):\n",
    "    async with async_session() as session:\n",
    "        author_subquery = select(Author.id, Author.model, Author.prompt).where(Author.model == model_name).subquery()\n",
    "        result = await session.execute(select(author_subquery.c.id, author_subquery.c.prompt, author_subquery.c.model).limit(10))\n",
    "        print(result.all())\n",
    "\n",
    "await test_author_subquery('gpt-4-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(976237, 'Kevin Kelley was the drummer for The Byrds in 1968.', 22086, 9),\n",
       " (976239, 'In 1968, the drummer for The Byrds was Michael Clarke.', 22086, 9),\n",
       " (976301, 'The West Wing won nine Emmy Awards during its debut season.', 4178, 9),\n",
       " (976304, 'The West Wing won 9 Emmy Awards during its debut season.', 4178, 9),\n",
       " (976492, 'Paraguay', 184843, 9),\n",
       " (976496, 'The Fortress of HumaitÃ¡ was built to increase the security of Paraguay.', 184843, 9),\n",
       " (976515, 'To gain membership at many gay bathhouses, customers generally need to pay a small fee and membership is typically open to any adult who seeks it.', 55883, 9),\n",
       " (976520, \"To gain membership at many gay bathhouses, customers typically need to provide a valid ID and pay a membership fee. Some bathhouses may also require agreeing to the establishment's rules and policies.\", 55883, 9),\n",
       " (976543, 'The 66th Venice International Film Festival.', 246790, 9),\n",
       " (976548, 'The film \"Mr. Nobody\" had its world premiere at the Venice Film Festival on 12 September 2009.', 246790, 9),\n",
       " (976580, 'David Icke predicted that the world would be devastated by tidal waves and earthquakes.', 177223, 9),\n",
       " (976589, 'David Icke did not specifically predict any natural disasters on his appearance on Wogan. Instead, he predicted a general impending global catastrophe, which he later associated with his theory of the world being controlled by shape-shifting reptilian humanoids.', 177223, 9),\n",
       " (976621, 'The first turnpike on the Hockliffe to Dunchurch stretch of Watling Street was approved by parliament in 1706.', 314537, 9),\n",
       " (976625, 'The first turnpike approved by Parliament on the Hockliffe to Dunchurch stretch of Watling Street was in 1706.', 314537, 9),\n",
       " (976656, 'Maniac Mansion was initially released for the Commodore 64 and Apple II in October 1987.', 441476, 9),\n",
       " (976659, 'Maniac Mansion was initially released for the Commodore 64 and the Apple II in October 1987.', 441476, 9),\n",
       " (976690, 'The purpose is for the player to demonstrate their capabilities, particularly in fulfilling the enforcer role, to secure a more permanent position on the team at the higher NHL level.', 421320, 9),\n",
       " (976695, \"The purpose of an American Hockey League (AHL) player getting a chance to play in an NHL game is to evaluate the player's readiness and potential to  ... (39 characters truncated) ... with experience, and develop their skills in a more competitive environment as part of the farm system strategy to groom talent for the major league.\", 421320, 9),\n",
       " (976726, 'Navy beans, broad beans, or string beans are commonly added to many kinds of borscht.', 463514, 9),\n",
       " (976730, 'Kidney beans and white beans are commonly added to many kinds of Borscht.', 463514, 9),\n",
       " (976752, 'The theme from Final Fantasy I played during card battles in Final Fantasy Fables: Chocobo Tales is the battle theme.', 372949, 9),\n",
       " (976755, 'The theme from Final Fantasy I played during card battles in Final Fantasy Fables: Chocobo Tales is \"Battle Scene.\"', 372949, 9),\n",
       " (976778, 'The most studied clinical prediction rule for assessing the probability of Deep Vein Thrombosis (DVT) is the Wells score.', 335296, 9),\n",
       " (976783, 'The Wells score for deep vein thrombosis (DVT) is the most studied clinical prediction rule for assessing the probability of DVT.', 335296, 9)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "async def fetch_answers_by_model(model_name):\n",
    "    async with async_session() as session:\n",
    "        # Define the subquery to fetch author IDs based on the model\n",
    "        author_subquery = select(Author.id).where(Author.model == model_name).subquery()\n",
    "\n",
    "        # Main query to fetch answers based on the author IDs from the subquery\n",
    "        results = await session.execute(\n",
    "            select(Answer.id, Answer.text, Answer.question_id, Answer.author_id)\n",
    "            .join(author_subquery, Answer.author_id == author_subquery.c.id)\n",
    "        )\n",
    "        return results.all()\n",
    "\n",
    "# Calling the function with the model name 'gpt-4-turbo'\n",
    "answers = await fetch_answers_by_model('gpt-4-turbo')\n",
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n"
     ]
    }
   ],
   "source": [
    "async def test_answer_subquery(model_name):\n",
    "    async with async_session() as session:\n",
    "        author_subquery = select(Author.id).where(Author.model == model_name).subquery()\n",
    "        answer_subquery = select(Answer.question_id).join(\n",
    "            author_subquery, Answer.author_id == author_subquery.c.id\n",
    "        ).group_by(\n",
    "            Answer.question_id, Answer.author_id\n",
    "        ).having(\n",
    "            func.count(Answer.id) >= 2,\n",
    "            func.sum(case((Answer.setting == 'ic', 1), else_=0)) > 0,\n",
    "            func.sum(case((Answer.setting == 'zs', 1), else_=0)) > 0\n",
    "        ).subquery()\n",
    "        result = await session.execute(select(answer_subquery.c.question_id))\n",
    "        return result.all()\n",
    "\n",
    "res = await test_answer_subquery('gpt-4-turbo')\n",
    "print(len(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = await fetch_data('gpt-4-turbo')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data = prepare_data(data)\n",
    "prepared_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await analyze_model('gpt-4-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ic', 859725), ('human', 36), ('zs', 1736)]\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import func\n",
    "\n",
    "async def get_setting_counts():\n",
    "    async with async_session() as session:\n",
    "        # Query to count each type of setting\n",
    "        results = await session.execute(\n",
    "            select(Answer.setting, func.count(Answer.setting))\n",
    "            .group_by(Answer.setting)\n",
    "        )\n",
    "        return results.all()\n",
    "\n",
    "# Example of how to run the function in an asyncio environment\n",
    "# Since we are in Jupyter, you can directly run this with `await` in a cell\n",
    "counts = await get_setting_counts()\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def perform_regression(df):\n",
    "    # Performing regression\n",
    "    X = df[['setting']]  # Independent variable\n",
    "    y = df['rating_value']  # Dependent variable\n",
    "    model = OLS(y, X).fit()\n",
    "    return model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
